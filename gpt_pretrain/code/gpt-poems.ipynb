{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"id":"L-7wLVJl_ATp","colab":{"base_uri":"https://localhost:8080/"},"outputId":"22417c7b-f246-4bd7-af7d-0d41e9db5af6","execution":{"iopub.status.busy":"2023-05-22T11:13:21.567607Z","iopub.execute_input":"2023-05-22T11:13:21.568508Z","iopub.status.idle":"2023-05-22T11:13:33.366037Z","shell.execute_reply.started":"2023-05-22T11:13:21.568472Z","shell.execute_reply":"2023-05-22T11:13:33.364906Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.28.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.11.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.3.23)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport random\nimport logging\nimport numpy as np\nimport io\nimport os\nimport re\nimport collections\nimport shutil\nfrom typing import Dict, List, Tuple\nimport glob\nimport argparse\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, IterableDataset, Dataset, RandomSampler, SequentialSampler\nfrom torch.utils.data.distributed import DistributedSampler\nfrom tqdm import tqdm, trange\n\nfrom transformers import AutoModelForCausalLM\nimport transformers\nfrom transformers import (\n    MODEL_WITH_LM_HEAD_MAPPING,\n    WEIGHTS_NAME,\n    AdamW,\n    AutoConfig,\n    AutoModelWithLMHead,\n    AutoTokenizer,\n    PreTrainedModel,\n    PreTrainedTokenizer,\n    get_linear_schedule_with_warmup,\n)\nfrom torch.utils.tensorboard import SummaryWriter","metadata":{"id":"ZHcHh9YCarmQ","execution":{"iopub.status.busy":"2023-05-22T11:13:33.368539Z","iopub.execute_input":"2023-05-22T11:13:33.368963Z","iopub.status.idle":"2023-05-22T11:13:44.214787Z","shell.execute_reply.started":"2023-05-22T11:13:33.368925Z","shell.execute_reply":"2023-05-22T11:13:44.213842Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"seed = 21\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)","metadata":{"id":"N9IOQb16DU0v","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ebfaeba2-cb25-4187-d413-1e8211c03363","execution":{"iopub.status.busy":"2023-05-22T11:13:44.216777Z","iopub.execute_input":"2023-05-22T11:13:44.217455Z","iopub.status.idle":"2023-05-22T11:13:44.230774Z","shell.execute_reply.started":"2023-05-22T11:13:44.217419Z","shell.execute_reply":"2023-05-22T11:13:44.229773Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7936ae847550>"},"metadata":{}}]},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nn_gpu = 1 if torch.cuda.is_available() else 0\ndevice","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WR6bWKaiDfoA","outputId":"04713cc3-bc0e-41de-af3a-82d94a9dff9c","execution":{"iopub.status.busy":"2023-05-22T11:13:44.234712Z","iopub.execute_input":"2023-05-22T11:13:44.235052Z","iopub.status.idle":"2023-05-22T11:13:44.273023Z","shell.execute_reply.started":"2023-05-22T11:13:44.235019Z","shell.execute_reply":"2023-05-22T11:13:44.271988Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"output_dir = \"/kaggle/working/gpt2_own_tokenizer\"\nlocal_rank = -1\nmodel_name = \"ai-forever/rugpt3medium_based_on_gpt2\"\npoems_path = \"/kaggle/input/poems-1/poems.txt\"","metadata":{"id":"ejP9qYyLbr4l","execution":{"iopub.status.busy":"2023-05-22T11:13:47.127006Z","iopub.execute_input":"2023-05-22T11:13:47.130335Z","iopub.status.idle":"2023-05-22T11:13:47.137319Z","shell.execute_reply.started":"2023-05-22T11:13:47.130292Z","shell.execute_reply":"2023-05-22T11:13:47.136127Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"#Код датасета и токенизатора","metadata":{"id":"bxtd82ApHYJD"}},{"cell_type":"code","source":"max_tokens_in_sample = 100","metadata":{"id":"fUBwVILvLWky","execution":{"iopub.status.busy":"2023-05-22T11:13:49.798073Z","iopub.execute_input":"2023-05-22T11:13:49.798509Z","iopub.status.idle":"2023-05-22T11:13:49.809598Z","shell.execute_reply.started":"2023-05-22T11:13:49.798476Z","shell.execute_reply":"2023-05-22T11:13:49.808749Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class StressedGptTokenizer(transformers.tokenization_utils.PreTrainedTokenizer):\n    def __init__(self, vocab_file=None, **kwargs):\n        super().__init__(**kwargs)\n        self.vocab = dict()\n\n        if vocab_file is not None:\n            with io.open(vocab_file, 'r', encoding='utf-8') as rdr:\n                for i, line in enumerate(rdr):\n                    self.vocab[line.strip()] = i\n            self.unk_token = '<unk>'\n            self.bos_token = '<s>'\n            self.eos_token = '</s>'\n            self.pad_token = '<pad>'\n            self.padding_side = 'right'\n            self.model_max_length = max_tokens_in_sample\n\n            self.id2str = dict((i, t) for t, i in self.vocab.items())\n            self.add_special_tokens({'pad_token': '<pad>', 'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '<nl>'})\n\n    def train(self, main_poetry_path, additional_prose_path, max_vocab_size):\n        self.vocab = {'<pad>': 0, '<s>': 1, '</s>': 2, '<unk>': 3, '<mask>': 4, '<nl>': 5}\n\n        data_units = set()\n        with io.open(main_poetry_path, 'r', encoding='utf-8') as rdr:\n            for line in rdr:\n                if not line.startswith('<|startoftext|>'):\n                    data_units.update(t for t in line.strip().split(' ') if t not in self.vocab)\n\n        if additional_prose_path is not None:\n            tokens2 = collections.Counter()\n            with io.open(additional_prose_path, 'r', encoding='utf-8') as rdr:\n                for line in rdr:\n                    if not line.startswith('<|startoftext|>'):\n                        for t in line.strip().split(' '):\n                            if t not in data_units and t not in self.vocab:\n                                if len(t) > 1:\n                                    tokens2[t] += 1\n\n                                # Берем символы из этого токена и добавляем их в словарь как отдельные элементы.\n                                for c in t:\n                                    data_units.add('##'+c)\n\n            if False:\n                max_additional_tokens = max_vocab_size - len(data_units) - len(self.vocab)\n                print('DEBUG@52 max_additional_tokens={}'.format(max_additional_tokens))\n                print('DEBUG@53 top-10 additional tokens: {}'.format(' '.join('{}({})'.format(unit, freq) for unit, freq in tokens2.most_common(n=10))))\n                data_units.update(unit for unit, _ in tokens2.most_common(n=max_additional_tokens))\n\n        self.vocab.update((t, i) for i, t in enumerate(data_units, start=len(self.vocab)))\n        self.id2str = dict((i, t) for t, i in self.vocab.items())\n\n    def save_pretrained(self, path):\n        with io.open(os.path.join(path, 'vocab.txt'), 'w', encoding='utf-8') as wrt:\n            for unit_text, _ in sorted(self.vocab.items(), key=lambda z: z[1]):\n                wrt.write(unit_text+'\\n')\n\n    @property\n    def vocab_size(self) -> int:\n        return len(self.vocab)\n    \n    def tokenize(self, text):\n        tokens = []\n        for t in re.split(r'\\s', text):\n            if t in self.vocab:\n                tokens.append(t)\n            else:\n                for c in t[::-1]:\n                    tokens.append('##'+c)\n        return tokens\n\n    def _convert_token_to_id(self, token):\n        return self.vocab.get(token, 3)  # self.unk_token_id\n\n    def is_special_token(self, token_id):\n        return 0 <= token_id <= 5\n\n    def decode(self, seq, clean_up_tokenization_spaces):\n        chunks = []\n        cur = 0\n        l = len(seq)\n        while cur < l:\n            token_id = seq[cur]\n            if isinstance(token_id, torch.Tensor):\n                token_id = token_id.item()\n\n            token = self.id2str[token_id]\n            if self.is_special_token(token_id):\n                chunks.append(token)\n                cur += 1\n            elif token.startswith('##'):\n                chunk = [token[2:]]  # отрезаем начальные ##\n                cur += 1\n                while cur < l:\n                    token_id = seq[cur]\n                    if isinstance(token_id, torch.Tensor):\n                        token_id = token_id.item()\n\n                    token = self.id2str[token_id]\n                    if token == '|':\n                        chunk_text = ''.join(chunk[::-1])\n                        chunks.append(chunk_text)\n                        chunks.append('|')\n                        chunk = []\n                        cur += 1\n                        break\n                    elif self.is_special_token(token_id):\n                        chunk_text = ''.join(chunk[::-1])\n                        chunks.append(chunk_text)\n                        chunks.append(token)\n                        chunk = []\n                        cur += 1\n                        break\n                    else:\n                        chunk.append(token[2:])  # отрезаем начальные ##\n                        cur += 1\n\n                if chunk:\n                    chunk_text = ''.join(chunk[::-1])\n                    chunks.append(chunk_text)\n            else:\n                chunks.append(token)\n                cur += 1\n                while cur < l:\n                    token_id = seq[cur]\n                    if isinstance(token_id, torch.Tensor):\n                        token_id = token_id.item()\n\n                    token = self.id2str[token_id]\n\n                    if token.startswith('##'):\n                        # считываем последовательность ##-токенов\n                        subseq = [token[2:]]\n                        while True:\n                            cur += 1\n                            if cur >= l:\n                                token = ''\n                                token_id = 0\n                                break\n\n                            token_id = seq[cur]\n                            if isinstance(token_id, torch.Tensor):\n                                token_id = token_id.item()\n                            token = self.id2str[token_id]\n                            if token.startswith('##'):\n                                subseq.append(token[2:])\n                            else:\n                                break\n\n                        token2 = ''.join(subseq[::-1])\n                        chunks.append(token2)\n\n                    if token == '|' or self.is_special_token(token_id):\n                        chunks.append(token)\n                        cur += 1\n                        break\n                    else:\n                        chunks.append(token)\n                        cur += 1\n\n        return ' '.join(chunks)\n\n    @staticmethod\n    def from_pretrained(path):\n        return StressedGptTokenizer(os.path.join(path, '/kaggle/input/poems-1/vocab.txt'))\n\n\n\ntokenizer = StressedGptTokenizer(vocab_file='/kaggle/input/poems-1/vocab.txt')","metadata":{"id":"2flMhWELfaAH","execution":{"iopub.status.busy":"2023-05-22T11:13:53.469122Z","iopub.execute_input":"2023-05-22T11:13:53.469540Z","iopub.status.idle":"2023-05-22T11:13:53.539171Z","shell.execute_reply.started":"2023-05-22T11:13:53.469505Z","shell.execute_reply":"2023-05-22T11:13:53.538272Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"logger = logging.getLogger(__name__)\nclass TextDataset(Dataset):\n    \"\"\"Текст из файла читается построчно, одна строка = один сэмпл\"\"\"\n\n    def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, block_size=512):\n        assert os.path.isfile(file_path)\n        # Here, we do not cache the features, operating under the assumption\n        # that we will soon use fast multithreaded tokenizers from the\n        # `tokenizers` repo everywhere =)\n        logger.info('Creating features from dataset file \"%s\", using line-by-line format and tokenizer=%s', file_path,\n                    tokenizer.__class__.__name__)\n\n        with open(file_path, encoding=\"utf-8\") as f:\n            lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n            poems = [line.split(\" | \") for line in lines]\n            context = []\n            for lines in poems:\n                for j in range(0,len(lines)-4,3):\n                    context.append(\" | \".join(lines[j:j+4]))\n        # Определим фактическую максимальную длину сэмплов\n        max_length = max(len(tokenizer.tokenize(line)) for line in lines) + 2\n        logger.info('max_length=%d', max_length)\n        self.examples = tokenizer.batch_encode_plus(context,\n                                                    add_special_tokens=True,\n                                                    max_length=max_tokens_in_sample, padding=True, \n\n                                                    truncation=True)[\"input_ids\"]\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, i):\n        return torch.tensor(self.examples[i], dtype=torch.long)","metadata":{"id":"adtdE4Rpaypb","execution":{"iopub.status.busy":"2023-05-22T11:13:58.182558Z","iopub.execute_input":"2023-05-22T11:13:58.183135Z","iopub.status.idle":"2023-05-22T11:13:58.194051Z","shell.execute_reply.started":"2023-05-22T11:13:58.183097Z","shell.execute_reply":"2023-05-22T11:13:58.193089Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"#Моделька","metadata":{"id":"bodTRWUxHmAR"}},{"cell_type":"code","source":"def load_and_cache_examples(args, tokenizer, evaluate=True):\n  file_path_mod = args.eval_data_file if evaluate else args.train_data_file\n  return TextDataset(tokenizer=tokenizer, file_path=file_path_mod)","metadata":{"id":"1M59836nzU0-","execution":{"iopub.status.busy":"2023-05-22T11:14:06.621782Z","iopub.execute_input":"2023-05-22T11:14:06.622686Z","iopub.status.idle":"2023-05-22T11:14:06.627725Z","shell.execute_reply.started":"2023-05-22T11:14:06.622651Z","shell.execute_reply":"2023-05-22T11:14:06.626772Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, prefix=\"\") -> Dict:\n    # Loop to handle MNLI double evaluation (matched, mis-matched)\n    eval_output_dir = output_dir\n\n    eval_dataset = load_and_cache_examples(args, tokenizer, evaluate=True)\n\n    if local_rank in [-1, 0]:\n        os.makedirs(eval_output_dir, exist_ok=True)\n\n    # Note that DistributedSampler samples randomly\n\n    def collate(examples: List[torch.Tensor]):\n        if tokenizer._pad_token is None:\n            return pad_sequence(examples, batch_first=True)\n        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n\n    if isinstance(eval_dataset, IterableDataset):\n        assert(local_rank == -1)\n        eval_dataloader = DataLoader(eval_dataset, sampler=None, batch_size=args.eval_batch_size, collate_fn=collate)\n    else:\n        eval_sampler = SequentialSampler(eval_dataset)\n        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate)\n\n    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n    logger.info(\"  Num examples = %d\", len(eval_dataset))\n    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n    eval_loss = 0.0\n    nb_eval_steps = 0\n    model.eval()\n\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        inputs, labels = mask_tokens(batch, tokenizer, args) if args.mlm else (batch, batch)\n        inputs = inputs.to(args.device)\n        labels = labels.to(args.device)\n\n        with torch.no_grad():\n            outputs = model(inputs, masked_lm_labels=labels) if args.mlm else model(inputs, labels=labels)\n            lm_loss = outputs[0]\n            eval_loss += lm_loss.mean().item()\n        nb_eval_steps += 1\n\n    eval_loss = eval_loss / nb_eval_steps\n    perplexity = torch.exp(torch.tensor(eval_loss))\n\n    result = {\"perplexity\": perplexity}\n\n    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n    with open(output_eval_file, \"w\") as writer:\n        logger.info(\"***** Eval results {} *****\".format(prefix))\n        for key in sorted(result.keys()):\n            logger.info(\"  %s = %s\", key, str(result[key]))\n            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n\n    return result","metadata":{"id":"7yP6aZmhPyc5","execution":{"iopub.status.busy":"2023-05-22T09:25:16.060025Z","iopub.execute_input":"2023-05-22T09:25:16.060404Z","iopub.status.idle":"2023-05-22T09:25:16.074529Z","shell.execute_reply.started":"2023-05-22T09:25:16.060374Z","shell.execute_reply":"2023-05-22T09:25:16.073452Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def train(args,train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer):\n    model = model.to(args.device)\n    history_loss = []\n    \"\"\" Train the model \"\"\"\n    if local_rank in [-1, 0]:\n        tb_writer = SummaryWriter(log_dir=args.output_dir)\n\n\n    def collate(examples: List[torch.Tensor]):\n        if tokenizer._pad_token is None:\n            return pad_sequence(examples, batch_first=True)\n        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n\n    if isinstance(train_dataset, IterableDataset):\n        assert(local_rank == -1)\n        train_dataloader = DataLoader(train_dataset, sampler=None, batch_size=args.train_batch_size, collate_fn=collate)\n    else:\n        train_sampler = RandomSampler(train_dataset) if local_rank == -1 else DistributedSampler(train_dataset)\n        train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate)\n\n    t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n\n    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n    model.resize_token_embeddings(len(tokenizer))\n\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n    )\n\n    # Check if saved optimizer or scheduler states exist\n    if (\n            args.model_name_or_path\n            and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n            and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n    ):\n        # Load in optimizer and scheduler states\n        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n\n    # Train!\n    logger.info(\"***** Running training *****\")\n    logger.info(\"  Num examples = %d\", len(train_dataset))\n    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n    logger.info(\"  Instantaneous batch size per GPU = %d\", args.train_batch_size)\n    logger.info(\n        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n        args.train_batch_size\n        * args.gradient_accumulation_steps\n        * (torch.distributed.get_world_size() if local_rank != -1 else 1),\n    )\n    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n    logger.info(\"  Total optimization steps = %d\", t_total)\n\n    global_step = 0\n    epochs_trained = 0\n    steps_trained_in_current_epoch = 0\n    # Check if continuing training from a checkpoint\n    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n        try:\n            # set global_step to gobal_step of last saved checkpoint from model path\n            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n            global_step = int(checkpoint_suffix)\n            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n\n            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n            logger.info(\"  Continuing training from global step %d\", global_step)\n            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n        except ValueError:\n            logger.info(\"  Starting fine-tuning.\")\n\n    tr_loss, logging_loss = 0.0, 0.0\n\n    model.zero_grad()\n    train_iterator = trange(\n        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=local_rank not in [-1, 0]\n    )\n    \n    for _ in train_iterator:\n        epoch_iterator = train_dataloader\n        for step, batch in enumerate(epoch_iterator):\n\n            # Skip past any already trained steps if resuming training\n            if steps_trained_in_current_epoch > 0:\n                steps_trained_in_current_epoch -= 1\n                continue\n\n            inputs, labels = mask_tokens(batch, tokenizer, args) if args.mlm else (batch, batch)\n            inputs = inputs.to(args.device)\n            labels = labels.to(args.device)\n            model.train()\n            outputs = model(inputs, labels=labels)\n            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n\n            loss.backward()\n\n            tr_loss += loss.item()\n            \n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n                optimizer.step()\n                scheduler.step()  # Update learning rate schedule\n                model.zero_grad()\n                global_step += 1\n                if global_step% args.save_steps == 0:\n                    history_loss.append(tr_loss / global_step)\n                    print(global_step ,tr_loss/ global_step)\n\n                if local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n                    # Log metrics\n                    if (\n                            local_rank == -1 and args.evaluate_during_training\n                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n                        results = evaluate(args, model, tokenizer)\n                        for key, value in results.items():\n                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n                            #print('DEBUG@558 EVAL step={} {}={}'.format(global_step, key, value))\n\n                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n                    #print('DEBUG@ step={} loss={}'.format(global_step, (tr_loss - logging_loss) / args.logging_steps))\n                    logging_loss = tr_loss\n\n                if local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n                    checkpoint_prefix = \"checkpoint\"\n                    # Save model checkpoint\n                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n                    os.makedirs(output_dir, exist_ok=True)\n                    model_to_save = (\n                        model.module if hasattr(model, \"module\") else model\n                    )  # Take care of distributed/parallel training\n                    model_to_save.save_pretrained(args.output_dir)\n                    tokenizer.save_pretrained(args.output_dir)\n\n                    torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n                    logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n\n                    _rotate_checkpoints(args, checkpoint_prefix)\n\n                    torch.save(optimizer.state_dict(), os.path.join(args.output_dir, \"optimizer.pt\"))\n                    torch.save(scheduler.state_dict(), os.path.join(args.output_dir, \"scheduler.pt\"))\n                    logger.info(\"Saving optimizer and scheduler states to %s\", args.output_dir)\n\n            if 0 < args.max_steps < global_step:\n                epoch_iterator.close()\n                break\n        if 0 < args.max_steps < global_step:\n            train_iterator.close()\n            break\n\n    if local_rank in [-1, 0]:\n        tb_writer.close()\n\n    return history_loss ,global_step, tr_loss / global_step, model","metadata":{"id":"it23wO9cPFe4","execution":{"iopub.status.busy":"2023-05-22T09:26:06.218602Z","iopub.execute_input":"2023-05-22T09:26:06.218966Z","iopub.status.idle":"2023-05-22T09:26:06.251144Z","shell.execute_reply.started":"2023-05-22T09:26:06.218936Z","shell.execute_reply":"2023-05-22T09:26:06.250043Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"#Парсеры","metadata":{"id":"q40ytSyBVvEj"}},{"cell_type":"code","source":"class params():\n  def __init__(self):\n    self.train_data_file = \"/kaggle/input/poems-1/poems.txt\"\n    self.output_dir = \"output_dir\"\n    self.eval_data_file = None \n    self.model_name_or_path = None \n    self.num_train_epochs = 1\n    self.device = device\n    self.block_size = -1\n    self.max_steps = -1\n    batch_size = 7\n    self.train_batch_size = batch_size\n    self.eval_batch_size = batch_size\n\n    self.cache_dir = None\n    self.do_train = True\n    self.do_eval = False\n    self.mlm = False\n    self.mlm_probability = 0.15\n    self.should_continue = False\n\n    self.learning_rate = 3e-5\n    self.weight_decay = 0.01\n    self.adam_epsilon = 1e-8\n    self.max_grad_norm = 1.0\n\n    self.logging_steps = 1000\n    self.save_steps = 1000\n\n    self.save_total_limit = None\n\n    self.evaluate_during_training = False\n\n    self.gradient_accumulation_steps = 1\n\n    self.eval_all_checkpoints = False\n\n    self.warmup_steps = 0\n","metadata":{"id":"_EL6yz2D3XHs","execution":{"iopub.status.busy":"2023-05-22T11:42:36.805640Z","iopub.execute_input":"2023-05-22T11:42:36.806526Z","iopub.status.idle":"2023-05-22T11:42:36.816051Z","shell.execute_reply.started":"2023-05-22T11:42:36.806490Z","shell.execute_reply":"2023-05-22T11:42:36.813288Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def main():\n    \n    args = params()\n\n    if args.eval_data_file is None and args.do_eval:\n        raise ValueError(\n            \"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n            \"or remove the --do_eval argument.\"\n        )\n        \n    if args.should_continue:\n        sorted_checkpoints = _sorted_checkpoints(args)\n        if len(sorted_checkpoints) == 0:\n            raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n        else:\n            args.model_name_or_path = sorted_checkpoints[-1]\n\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO if local_rank in [-1, 0] else logging.WARN,\n    )\n\n    \n    config = AutoConfig.from_pretrained(model_name, cache_dir=args.cache_dir)\n\n    logging.info('StressedGptTokenizer from \"%s\" will be used')\n    #tokenizer = StressedGptTokenizer(vocab_file='/kaggle/input/poems-1/vocab.txt')\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    tokenizer.add_special_tokens({'pad_token': '<pad>', 'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '<nl>'})\n\n    if args.block_size <= 0:\n        args.block_size = max_tokens_in_sample\n\n    model = AutoModelForCausalLM.from_pretrained(model_name)\n    \n    if local_rank == 0:\n        torch.distributed.barrier()\n        # End of barrier to make sure only the first process in distributed training download model & vocab\n\n    logger.info(\"Training/evaluation parameters %s\", args)\n    \n    # Training\n    if args.do_train:\n        if local_rank not in [-1, 0]:\n            torch.distributed.barrier()\n            # Barrier to make sure only the first process in distributed\n            # training process the dataset, and the others will use the cache\n        \n        train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False)\n\n        history_loss ,global_step, tr_loss, model = train(args, train_dataset, model, tokenizer)\n        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n    print(global_step, tr_loss)\n    model.save_pretrained(output_dir+\"/model_last_version\")\n    tokenizer.save_pretrained(output_dir+\"/model_last_version\")\n    config.save_pretrained(output_dir+\"/model_last_version\")\n    return history_loss , model, tokenizer\n    ","metadata":{"id":"LOFstEvpRCni","execution":{"iopub.status.busy":"2023-05-22T12:54:27.729933Z","iopub.execute_input":"2023-05-22T12:54:27.730414Z","iopub.status.idle":"2023-05-22T12:54:27.742620Z","shell.execute_reply.started":"2023-05-22T12:54:27.730378Z","shell.execute_reply":"2023-05-22T12:54:27.741593Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"hist, model, tokenizer = main()","metadata":{"id":"4dlzMHPjzDTn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a4dc882b-cd66-4890-c454-e01d94451a35","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Загрузка модели и токенизатора","metadata":{}},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained('/kaggle/input/model-data/').to(device)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T11:33:57.666492Z","iopub.execute_input":"2023-05-22T11:33:57.666871Z","iopub.status.idle":"2023-05-22T11:34:07.969682Z","shell.execute_reply.started":"2023-05-22T11:33:57.666841Z","shell.execute_reply":"2023-05-22T11:34:07.968749Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"args = params()\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.add_special_tokens({'pad_token': '<pad>', 'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '<nl>'})","metadata":{"execution":{"iopub.status.busy":"2023-05-22T11:43:09.223768Z","iopub.execute_input":"2023-05-22T11:43:09.224363Z","iopub.status.idle":"2023-05-22T11:43:16.675745Z","shell.execute_reply.started":"2023-05-22T11:43:09.224324Z","shell.execute_reply":"2023-05-22T11:43:16.674744Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/674 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cb66c27879f48679adc64945c83b314"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.61M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3504c33908d46f99df76e9738655a78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/1.27M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17a4c94277514805ade97b98f7e7a97d"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"cell_type":"code","source":"def generate_output(tokenizer, model, context, num_return_sequences=5, temperature=0.5):\n        beam_k = 0\n        beam_p = 1.0\n        typical_p = 0.6\n        repetition_penalty = 1.2\n        prompt_text = \"<s>\" + context + ' #'\n        stop_token = \"</s>\"\n        length = 50\n\n        encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n        encoded_prompt = encoded_prompt.to(device)\n\n        output_sequences = model.generate(encoded_prompt, \n                        do_sample=True,\n                        \n                        temperature=1.0,\n                        top_k = 100,\n                        top_p=1.0,\n                        max_length=length,\n                        num_return_sequences=num_return_sequences\n                        )\n        if len(output_sequences.shape) > 2:\n            output_sequences.squeeze_()\n\n        generated_sequences = set()\n        for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n            #print(\"ruGPT2Large:\".format(generated_sequence_idx + 1))\n            generated_sequence = generated_sequence.tolist()\n\n            # Decode text\n            text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n            # Remove all text after the stop token\n            if stop_token in text:\n                text = text[: text.find(stop_token)]\n\n            # Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing\n            total_sequence = text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)):]\n\n            if '#' in total_sequence:\n                total_sequence = total_sequence[: total_sequence.find('#')]\n\n            total_sequence = total_sequence.strip()\n            generated_sequences.add(total_sequence)\n\n        return list(generated_sequences)\n","metadata":{"id":"iQx6Fauf1h4t","execution":{"iopub.status.busy":"2023-05-22T11:43:45.442376Z","iopub.execute_input":"2023-05-22T11:43:45.442809Z","iopub.status.idle":"2023-05-22T11:43:45.458455Z","shell.execute_reply.started":"2023-05-22T11:43:45.442775Z","shell.execute_reply":"2023-05-22T11:43:45.457398Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"generate_output(tokenizer, model, \"какое дикое ущелье\")","metadata":{"execution":{"iopub.status.busy":"2023-05-22T12:50:53.999276Z","iopub.execute_input":"2023-05-22T12:50:54.000033Z","iopub.status.idle":"2023-05-22T12:50:54.938661Z","shell.execute_reply.started":"2023-05-22T12:50:53.999996Z","shell.execute_reply":"2023-05-22T12:50:54.937633Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"execution_count":83,"output_type":"execute_result","data":{"text/plain":"['| оно  только лишь одна сторона сна | и лишь начало дороги между нас | мы стоим в нем и я в нем стою и мы<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n 'сразу наискосок | и все же как ни странно все же  | как ни странно все же снова дорога<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n 'здесь | на горизонте облака  вот | я все ждал как люди они придут<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n 'которое | а ну иди сюда | в этом ущелье на кочках на льду<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n '| и как много жизни черной | скрыто в той черноте бездонной | не знаю какая ночь<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']"},"metadata":{}}]}]}